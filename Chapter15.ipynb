{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cd52b55",
   "metadata": {},
   "source": [
    "# Chapter 15: NLP Analysis of Large Text Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b98f5b",
   "metadata": {},
   "source": [
    "## 15.1 Loading Online Forum Discussions Using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b35db45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups = fetch_20newsgroups(remove=('headers', 'footers'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2c09254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "# Printing the names of all 20 newsgroups\n",
    "print(newsgroups.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e020fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n"
     ]
    }
   ],
   "source": [
    "# Printing the first newsgroup post\n",
    "print(newsgroups.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31d6e16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The post at index 0 first appeared in the 'rec.autos' group\n"
     ]
    }
   ],
   "source": [
    "# Printing the newsgroup name at index 0\n",
    "origin = newsgroups.target_names[newsgroups.target[0]]\n",
    "print(f\"The post at index 0 first appeared in the '{origin}' group\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82a5b0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our dataset contains 11314 newsgroup posts\n"
     ]
    }
   ],
   "source": [
    "# Counting the number of newsgroup posts\n",
    "dataset_size = len(newsgroups.data)\n",
    "print(f\"Our dataset contains {dataset_size} newsgroup posts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f333888b",
   "metadata": {},
   "source": [
    "## 15.2 Vectorizing Documents Using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70d974c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initalizing a CountVectorizer object\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a4167be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 108644)\t4\n",
      "  (0, 110106)\t1\n",
      "  (0, 57577)\t2\n",
      "  (0, 24398)\t2\n",
      "  (0, 79534)\t1\n",
      "  (0, 100942)\t1\n",
      "  (0, 37154)\t1\n",
      "  (0, 45141)\t1\n",
      "  (0, 70570)\t1\n",
      "  (0, 78701)\t2\n",
      "  (0, 101084)\t4\n",
      "  (0, 32499)\t4\n",
      "  (0, 92157)\t1\n",
      "  (0, 100827)\t6\n",
      "  (0, 79461)\t1\n",
      "  (0, 39275)\t1\n",
      "  (0, 60326)\t2\n",
      "  (0, 42332)\t1\n",
      "  (0, 96432)\t1\n",
      "  (0, 67137)\t1\n",
      "  (0, 101732)\t1\n",
      "  (0, 27703)\t1\n",
      "  (0, 49871)\t2\n",
      "  (0, 65338)\t1\n",
      "  (0, 14106)\t1\n",
      "  :\t:\n",
      "  (11313, 55901)\t1\n",
      "  (11313, 93448)\t1\n",
      "  (11313, 97535)\t1\n",
      "  (11313, 93393)\t1\n",
      "  (11313, 109366)\t1\n",
      "  (11313, 102215)\t1\n",
      "  (11313, 29148)\t1\n",
      "  (11313, 26901)\t1\n",
      "  (11313, 94401)\t1\n",
      "  (11313, 89686)\t1\n",
      "  (11313, 80827)\t1\n",
      "  (11313, 72219)\t1\n",
      "  (11313, 32984)\t1\n",
      "  (11313, 82912)\t1\n",
      "  (11313, 99934)\t1\n",
      "  (11313, 96505)\t1\n",
      "  (11313, 72102)\t1\n",
      "  (11313, 32981)\t1\n",
      "  (11313, 82692)\t1\n",
      "  (11313, 101854)\t1\n",
      "  (11313, 66399)\t1\n",
      "  (11313, 63405)\t1\n",
      "  (11313, 61366)\t1\n",
      "  (11313, 7462)\t1\n",
      "  (11313, 109600)\t1\n"
     ]
    }
   ],
   "source": [
    "# TF Matrix: Stores the counds of words (columns) across all texts (rows)\n",
    "# Computing a TF matrix with scikit-learn\n",
    "tf_matrix = vectorizer.fit_transform(newsgroups.data)\n",
    "print(tf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3fbd4795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse._csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "# Checking the data type of tf_matrix\n",
    "print(type(tf_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f2c13dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Compressed Sparse Row (CSR): Storage format for compressing matrices that are composed mostly of zeros\n",
    "# Sparse Matrix: A matrix with mostly 0s; compression occurs when the 0s are removed, leaving just the data\n",
    "# Convertinga CSR matrix to a NumPy array\n",
    "tf_np_matrix = tf_matrix.toarray()\n",
    "print(tf_np_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c78c261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our collection of 11314 newsgroup posts contain a total of 114751 unique words\n"
     ]
    }
   ],
   "source": [
    "# Checking the vocabulary size\n",
    "assert tf_np_matrix.shape == tf_matrix.shape\n",
    "num_posts, vocabulary_size = tf_np_matrix.shape\n",
    "print(f\"Our collection of {num_posts} newsgroup posts contain a total of {vocabulary_size} unique words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "131b833b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The newsgroup in row 0 contains 64 unique words.\n",
      "The actual word counts map to the following column indices:\n",
      " [ 14106  15549  22088  23323  24398  27703  29357  30093  30629  32194\n",
      "  32305  32499  37154  39275  42332  42333  43643  45089  45141  49871\n",
      "  49881  50165  54442  55453  57577  58321  58842  60116  60326  64083\n",
      "  65338  67137  67140  68931  69080  70570  72915  75280  78264  78701\n",
      "  79055  79461  79534  82759  84398  87690  89161  92157  93304  95225\n",
      "  96145  96432 100406 100827 100942 101084 101732 108644 109086 109254\n",
      " 109294 110106 112936 113262]\n"
     ]
    }
   ],
   "source": [
    "# Counting the unique words in the car post\n",
    "import numpy as np\n",
    "tf_vector = tf_np_matrix[0]\n",
    "non_zero_indices = np.flatnonzero(tf_vector)\n",
    "num_unique_words = non_zero_indices.size\n",
    "print(f\"The newsgroup in row 0 contains {num_unique_words} unique words.\")\n",
    "print(f\"The actual word counts map to the following column indices:\\n {non_zero_indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3d73695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['60s', '70s', 'addition', 'all', 'anyone', 'be', 'body', 'bricklin', 'bumper', 'called', 'can', 'car', 'could', 'day', 'door', 'doors', 'early', 'engine', 'enlighten', 'from', 'front', 'funky', 'have', 'history', 'if', 'in', 'info', 'is', 'it', 'know', 'late', 'looked', 'looking', 'made', 'mail', 'me', 'model', 'name', 'of', 'on', 'or', 'other', 'out', 'please', 'production', 'really', 'rest', 'saw', 'separate', 'small', 'specs', 'sports', 'tellme', 'the', 'there', 'this', 'to', 'was', 'were', 'whatever', 'where', 'wondering', 'years', 'you']\n"
     ]
    }
   ],
   "source": [
    "# Printing the unique words in the car post\n",
    "words = vectorizer.get_feature_names_out()\n",
    "unique_words = [words[i] for i in non_zero_indices]\n",
    "print(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5991f6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Word  Count\n",
      "   the      6\n",
      "  this      4\n",
      "   was      4\n",
      "   car      4\n",
      "    if      2\n",
      "    is      2\n",
      "    it      2\n",
      "  from      2\n",
      "    on      2\n",
      "anyone      2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = {'Word': unique_words,\n",
    "        'Count': tf_vector[non_zero_indices]}\n",
    "\n",
    "df = pd.DataFrame(data).sort_values('Count', ascending=False)\n",
    "print(df[:10].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e23ace49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Words: Words like \"the\", \"this\", \"was\", etc. that are not unique words with meaning\n",
    "# Removing stop words during vectorization\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "tf_matrix = vectorizer.fit_transform(newsgroups.data)\n",
    "assert tf_matrix.shape[1] < 114751\n",
    "\n",
    "words = vectorizer.get_feature_names_out()\n",
    "for common_word in ['the', 'this', 'was', 'if', 'it', 'on']:\n",
    "    assert common_word not in words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3396f461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After stop-word deletion, 34 unique words remain.\n",
      "      Word  Count\n",
      "       car      4\n",
      "       60s      1\n",
      "       saw      1\n",
      "   looking      1\n",
      "      mail      1\n",
      "     model      1\n",
      "production      1\n",
      "    really      1\n",
      "      rest      1\n",
      "  separate      1\n"
     ]
    }
   ],
   "source": [
    "# Reprinting the top words after stop-word deletion\n",
    "tf_np_matrix = tf_matrix.toarray()\n",
    "tf_vector = tf_np_matrix[0]\n",
    "non_zero_indices = np.flatnonzero(tf_vector)\n",
    "unique_words = [words[index] for index in non_zero_indices]\n",
    "data = {'Word': unique_words,\n",
    "        'Count': tf_vector[non_zero_indices]}\n",
    "\n",
    "df = pd.DataFrame(data).sort_values('Count', ascending=False)\n",
    "print(f\"After stop-word deletion, {df.shape[0]} unique words remain.\")\n",
    "print(df[:10].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8599f17c",
   "metadata": {},
   "source": [
    "## 15.3 Ranking Words by Both Post Frequency and Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf087652",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
